{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -------------------------------------------------------------------------------------------\n",
    "#  Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n",
    "#  -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase grounding\n",
    "\n",
    "This notebook demonstrates the usage of the BioViL image and text models in a multi-modal phrase grounding setting.\n",
    "Given a chest X-ray and a radiology text phrase, the joint model grounds the phrase in the image, i.e., highlights the regions of the image that share features similar to the phrase.\n",
    "\n",
    "Please refer to the ECCV'22 manuscript for further details:\n",
    "\n",
    "> Boecking, B., Usuyama, N. et al. (2022). Making the Most of Text Semantics to Improve Biomedical Visionâ€“Language Processing. In: Avidan, S., Brostow, G., CissÃ©, M., Farinella, G.M., Hassner, T. (eds) Computer Vision â€“ ECCV 2022. ECCV 2022. Lecture Notes in Computer Science, vol 13696. Springer, Cham. [https://doi.org/10.1007/978-3-031-20059-5_1](https://doi.org/10.1007/978-3-031-20059-5_1)\n",
    "\n",
    "The notebook can also be run on Binder without the need of any coding or local installation:\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/hi-ml/HEAD?labpath=hi-ml-multimodal%2Fnotebooks%2Fphrase_grounding.ipynb)\n",
    "\n",
    "This demo is solely for research evaluation purposes, not intended to be a medical product or clinical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's first install the `hi-ml-multimodal` Python package, which will allow us to import the `health_multimodal` Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "pip_source = \"hi-ml-multimodal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install {pip_source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/ozoktay/workspace/hi-ml/hi-ml-multimodal/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from health_multimodal.text import get_bert_inference\n",
    "from health_multimodal.image import get_image_inference\n",
    "from health_multimodal.vlp import ImageTextInferenceEngine\n",
    "from health_multimodal.common.visualization import plot_phrase_grounding_similarity_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multimodal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text and image models from [Hugging Face ðŸ¤—](https://aka.ms/biovil-models) and instantiate the inference engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inference = get_bert_inference(\"biovil_t_bert\")\n",
    "image_inference = get_image_inference(\"biovil_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the joint inference engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text_inference = ImageTextInferenceEngine(\n",
    "    image_inference_engine=image_inference,\n",
    "    text_inference_engine=text_inference,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_text_inference.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phrase_grounding(image_path: Path, text_prompt: str, bboxes: List[Tuple[float, float, float, float]]) -> None:\n",
    "    similarity_map = image_text_inference.get_similarity_map_from_raw_data(\n",
    "        image_path=image_path,\n",
    "        query_text=text_prompt,\n",
    "        interpolation=\"bilinear\",\n",
    "    )\n",
    "    plot_phrase_grounding_similarity_map(\n",
    "        image_path=image_path,\n",
    "        similarity_map=similarity_map,\n",
    "        bboxes=bboxes\n",
    "    )\n",
    "\n",
    "def plot_phrase_grounding_from_url(image_url: str, text_prompt: str, bboxes: List[Tuple[float, float, float, float]]) -> None:\n",
    "    image_path = Path(tempfile.tempdir, \"downloaded_chest_xray.jpg\")\n",
    "    !curl -s -L -o {image_path} {image_url}\n",
    "    plot_phrase_grounding(image_path, text_prompt, bboxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We will run inference on a chest X-ray from [Open-i](https://openi.nlm.nih.gov/detailedresult?img=CXR111_IM-0076-1001&req=4), but any other chest X-ray image in DICOM or JPEG format can be used for research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://openi.nlm.nih.gov/imgs/512/242/1445/CXR1445_IM-0287-4004.png\"\n",
    "text_prompt = \"Left basilar consolidation seen\"\n",
    "\n",
    "# Ground-truth bounding box annotation(s) for the input text prompt.\n",
    "bboxes = [[306, 168, 124, 101]]\n",
    "\n",
    "# Phrase-grounding experiment\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"Phrase Grounding Experiment: \\n\\n\"\n",
    "                 f\"Ground-truth bounding box annotation(s) for the phrase \\\"{text_prompt}\\\" is shown in the middle figure (in black).\"))\n",
    "\n",
    "plot_phrase_grounding_from_url(image_url, text_prompt, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('multimodal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3ed4ebeb13ecf0e023e91854a12f6980e67b335857d302451652e7fbb2d2298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
